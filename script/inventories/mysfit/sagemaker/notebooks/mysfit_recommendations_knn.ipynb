{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## The Mysfits Recommendations Notebook\n",
    "\n",
    "The code required to use the sample data and build a machine learning model has already been written and is contained within the following cells below in this notebook.  \n",
    "\n",
    "It is your task to read over the documentation to gain an understanding of the steps taken, and get familiar with interacting with this notebook in order to curate data, build and train and machine learning model, and deploy that model for use by our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAGEMAKER_ENDPOINT_NAME = \"mysfit_recommendation_knn_endpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Downloading the Sample Data\n",
    "---\n",
    "\n",
    "The S3 data set is the responses to a questionnaire \"which Mysfit is their favorite?\" by nearly one million imaginary users of the Mythical Mysfits. Try the questionnaire for yourself at [Mythical Mysfits website](http://www.mythicalmysfits.com).\n",
    "\n",
    "The result of the five question questionnaire and a favorite mysfit is a CSV file where each line contains 6 comma separate values (Example: `1,0,2,7,0,11`). Each possible questionnaire response and the chosen mysfit is mapped to a numerical value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2020-03-25 21:32:12--  https://s3.amazonaws.com/mysfit-recommendation-training-data/mysfit-preferences.csv.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.26.254\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.26.254|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3504843 (3.3M) [text/csv]\n",
      "Saving to: ‘mysfit-preferences.csv.gz’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1%  142K 24s\n",
      "    50K .......... .......... .......... .......... ..........  2%  405K 16s\n",
      "   100K .......... .......... .......... .......... ..........  4%  222K 15s\n",
      "   150K .......... .......... .......... .......... ..........  5%  212K 15s\n",
      "   200K .......... .......... .......... .......... ..........  7%  249K 14s\n",
      "   250K .......... .......... .......... .......... ..........  8% 1.78M 12s\n",
      "   300K .......... .......... .......... .......... .......... 10%  204K 12s\n",
      "   350K .......... .......... .......... .......... .......... 11%  257K 12s\n",
      "   400K .......... .......... .......... .......... .......... 13% 1.68M 11s\n",
      "   450K .......... .......... .......... .......... .......... 14%  195K 11s\n",
      "   500K .......... .......... .......... .......... .......... 16%  297K 11s\n",
      "   550K .......... .......... .......... .......... .......... 17% 1.62M 10s\n",
      "   600K .......... .......... .......... .......... .......... 18%  255K 10s\n",
      "   650K .......... .......... .......... .......... .......... 20%  792K 9s\n",
      "   700K .......... .......... .......... .......... .......... 21%  228K 9s\n",
      "   750K .......... .......... .......... .......... .......... 23%  244K 9s\n",
      "   800K .......... .......... .......... .......... .......... 24%  263K 9s\n",
      "   850K .......... .......... .......... .......... .......... 26%  924K 8s\n",
      "   900K .......... .......... .......... .......... .......... 27%  260K 8s\n",
      "   950K .......... .......... .......... .......... .......... 29%  447K 8s\n",
      "  1000K .......... .......... .......... .......... .......... 30%  316K 8s\n",
      "  1050K .......... .......... .......... .......... .......... 32%  261K 8s\n",
      "  1100K .......... .......... .......... .......... .......... 33%  455K 7s\n",
      "  1150K .......... .......... .......... .......... .......... 35%  359K 7s\n",
      "  1200K .......... .......... .......... .......... .......... 36%  245K 7s\n",
      "  1250K .......... .......... .......... .......... .......... 37%  450K 7s\n",
      "  1300K .......... .......... .......... .......... .......... 39%  364K 7s\n",
      "  1350K .......... .......... .......... .......... .......... 40%  249K 7s\n",
      "  1400K .......... .......... .......... .......... .......... 42%  543K 6s\n",
      "  1450K .......... .......... .......... .......... .......... 43%  319K 6s\n",
      "  1500K .......... .......... .......... .......... .......... 45%  250K 6s\n",
      "  1550K .......... .......... .......... .......... .......... 46%  898K 6s\n",
      "  1600K .......... .......... .......... .......... .......... 48%  259K 6s\n",
      "  1650K .......... .......... .......... .......... .......... 49%  265K 6s\n",
      "  1700K .......... .......... .......... .......... .......... 51% 1.07M 5s\n",
      "  1750K .......... .......... .......... .......... .......... 52%  257K 5s\n",
      "  1800K .......... .......... .......... .......... .......... 54%  535K 5s\n",
      "  1850K .......... .......... .......... .......... .......... 55%  322K 5s\n",
      "  1900K .......... .......... .......... .......... .......... 56%  244K 5s\n",
      "  1950K .......... .......... .......... .......... .......... 58%  559K 4s\n",
      "  2000K .......... .......... .......... .......... .......... 59%  328K 4s\n",
      "  2050K .......... .......... .......... .......... .......... 61%  265K 4s\n",
      "  2100K .......... .......... .......... .......... .......... 62%  615K 4s\n",
      "  2150K .......... .......... .......... .......... .......... 64%  290K 4s\n",
      "  2200K .......... .......... .......... .......... .......... 65%  289K 4s\n",
      "  2250K .......... .......... .......... .......... .......... 67%  493K 3s\n",
      "  2300K .......... .......... .......... .......... .......... 68%  296K 3s\n",
      "  2350K .......... .......... .......... .......... .......... 70%  369K 3s\n",
      "  2400K .......... .......... .......... .......... .......... 71%  475K 3s\n",
      "  2450K .......... .......... .......... .......... .......... 73%  290K 3s\n",
      "  2500K .......... .......... .......... .......... .......... 74%  539K 3s\n",
      "  2550K .......... .......... .......... .......... .......... 75%  536K 2s\n",
      "  2600K .......... .......... .......... .......... .......... 77%  451K 2s\n",
      "  2650K .......... .......... .......... .......... .......... 78%  296K 2s\n",
      "  2700K .......... .......... .......... .......... .......... 80%  623K 2s\n",
      "  2750K .......... .......... .......... .......... .......... 81%  294K 2s\n",
      "  2800K .......... .......... .......... .......... .......... 83%  837K 2s\n",
      "  2850K .......... .......... .......... .......... .......... 84%  299K 2s\n",
      "  2900K .......... .......... .......... .......... .......... 86%  847K 1s\n",
      "  2950K .......... .......... .......... .......... .......... 87%  301K 1s\n",
      "  3000K .......... .......... .......... .......... .......... 89%  928K 1s\n",
      "  3050K .......... .......... .......... .......... .......... 90%  350K 1s\n",
      "  3100K .......... .......... .......... .......... .......... 92%  629K 1s\n",
      "  3150K .......... .......... .......... .......... .......... 93%  350K 1s\n",
      "  3200K .......... .......... .......... .......... .......... 94%  727K 0s\n",
      "  3250K .......... .......... .......... .......... .......... 96%  508K 0s\n",
      "  3300K .......... .......... .......... .......... .......... 97%  528K 0s\n",
      "  3350K .......... .......... .......... .......... .......... 99% 1003K 0s\n",
      "  3400K .......... .......... ..                              100% 4.72M=9.6s\n",
      "\n",
      "2020-03-25 21:32:23 (357 KB/s) - ‘mysfit-preferences.csv.gz’ saved [3504843/3504843]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget 'https://s3.amazonaws.com/mysfit-recommendation-training-data/mysfit-preferences.csv.gz'\n",
    "mkdir -p /tmp/mysfit/raw\n",
    "mv mysfit-preferences.csv.gz /tmp/mysfit/raw/mysfit-preferences.csv.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2: Data Preparation\n",
    "---\n",
    "\n",
    "## Pre-Processing the Data\n",
    "Now that we have the raw data, let's process it. \n",
    "We'll first load the data into numpy arrays, and randomly split it into train and test with a 90/10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "a58fdf0d-32fb-4690-add3-433cc721773d"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw data from /tmp/mysfit/raw/mysfit-preferences.csv.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_dir = \"/tmp/mysfit/\"\n",
    "processed_subdir = \"standardized\"\n",
    "raw_data_file = os.path.join(data_dir, \"raw\", \"mysfit-preferences.csv.gz\")\n",
    "train_features_file = os.path.join(data_dir, processed_subdir, \"train/csv/features.csv\")\n",
    "train_labels_file = os.path.join(data_dir, processed_subdir, \"train/csv/labels.csv\")\n",
    "test_features_file = os.path.join(data_dir, processed_subdir, \"test/csv/features.csv\")\n",
    "test_labels_file = os.path.join(data_dir, processed_subdir, \"test/csv/labels.csv\")\n",
    "\n",
    "# read raw data\n",
    "print(\"Reading raw data from {}\".format(raw_data_file))\n",
    "raw = np.loadtxt(raw_data_file, delimiter=',')\n",
    "\n",
    "# split into train/test with a 90/10 split\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(raw)\n",
    "train_size = int(0.9 * raw.shape[0])\n",
    "train_features = raw[:train_size,  :-1]\n",
    "train_labels   = raw[:train_size,   -1]\n",
    "test_features  = raw[ train_size:, :-1]\n",
    "test_labels    = raw[ train_size:,  -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4   5\n",
       "0  8  3  5  7  5   6\n",
       "1  1  7  6  6  4  10\n",
       "2  6  8  1  2  5   0\n",
       "3  4  8  7  8  1  10\n",
       "4  7  5  8  8  8   7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(raw_data_file, header=None)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8  3  5  7  5.1   6\n",
      "0  1  7  6  6    4  10\n",
      "1  6  8  1  2    5   0\n",
      "2  4  8  7  8    1  10\n",
      "3  7  5  8  8    8   7\n",
      "4  4  7  6  5    2   2\n",
      "train_features shape =  (865417, 5)\n",
      "train_labels shape =  (865417,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train.head(5))\n",
    "print('train_features shape = ', train_features.shape)\n",
    "print('train_labels shape = ', train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Amazon S3\n",
    "Now, since typically the dataset will be large and located in Amazon S3, let's write the data to Amazon S3 in recordio-protobuf format. We first create an io buffer wrapping the data, next we upload it to Amazon S3. Notice that the choice of bucket and prefix should change for different users and different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "print('train_features shape = ', train_features.shape)\n",
    "print('train_labels shape = ', train_labels.shape)\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, train_features, train_labels)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket() # modify to your bucket name\n",
    "prefix = 'mysfit-recommendation-dataset'\n",
    "key = 'recordio-pb-data'\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to provide test data. This way we can get an evaluation of the performance of the model from the training logs. In order to use this capability let's upload the test data to Amazon S3 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_features shape = ', test_features.shape)\n",
    "print('test_labels shape = ', test_labels.shape)\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, test_features, test_labels)\n",
    "buf.seek(0)\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test', key)).upload_fileobj(buf)\n",
    "s3_test_data = 's3://{}/{}/test/{}'.format(bucket, prefix, key)\n",
    "print('uploaded test data location: {}'.format(s3_test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Training\n",
    "---\n",
    "\n",
    "First, the SageMaker training process called **estimator** takes a labeled dataset and hyper-parameters and outputs a model. Second, set up an **endpoint** which is a web service that returns a prediction(s) to a request.\n",
    "\n",
    "When setting up the estimator we specify:\n",
    "* the location of the training data in S3, \n",
    "* the path (again in S3) to the model output directory, \n",
    "* generic hyper-parameters such as the machine type to use during the training process, and \n",
    "* kNN-specific hyper-parameters such as the index type, etc. \n",
    "\n",
    "Once the estimator is initialized, we call its **fit** method to do the actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "# Docker image of the ML algorithm\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "\n",
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # Set up the KNN estimator\n",
    "    knn = sagemaker.estimator.Estimator(\n",
    "        get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "        get_execution_role(),\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.m5.2xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session()\n",
    "    )\n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "    \n",
    "    # train a model. fit_input contains the locations of the train and test data\n",
    "    fit_input = {'train': s3_train_data}\n",
    "    \n",
    "    # When a test set is provided the training job doesn't just produce a model \n",
    "    # but also applies it to the test set and reports the accuracy. \n",
    "    # In the logs you can view the accuracy of the model on the test set.\n",
    "    if s3_test_data is not None:\n",
    "        fit_input['test'] = s3_test_data\n",
    "    knn.fit(fit_input)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run the actual training job. For now, we stick to default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'feature_dim': 5,\n",
    "    'k': 10,\n",
    "    'sample_size': 100000,\n",
    "    'predictor_type': 'classifier' \n",
    "}\n",
    "output_path = 's3://' + bucket + '/' + prefix + '/default_example/output'\n",
    "knn_estimator = trained_estimator_from_hyperparams(\n",
    "    s3_train_data, \n",
    "    hyperparams, \n",
    "    output_path, \n",
    "    s3_test_data=s3_test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Deploying the Model to a SageMaker Endpoint\n",
    "---\n",
    "\n",
    "## Setting up the endpoint\n",
    "\n",
    "Now that we have a trained model, we are ready to run inference. The **knn_estimator** object above contains all the information we need for hosting the model. \n",
    "\n",
    "Below we provide a convenience function that given an estimator, sets up and endpoint that hosts the model. Other than the estimator object, we provide it with a name (string) for the estimator, and an **instance_type**. The **instance_type** is the machine type that will host the model. It is not restricted in any way by the parameter settings of the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor_from_estimator(knn_estimator, estimator_name, instance_type, endpoint_name=None): \n",
    "    knn_predictor = knn_estimator.deploy(initial_instance_count=1, instance_type=instance_type,\n",
    "                                        endpoint_name=endpoint_name)\n",
    "    knn_predictor.content_type = 'text/csv'\n",
    "    knn_predictor.serializer = csv_serializer\n",
    "    knn_predictor.deserializer = json_deserializer\n",
    "    return knn_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "instance_type = 'ml.m4.xlarge'\n",
    "model_name = 'knn_%s'% instance_type\n",
    "#endpoint_name = 'knn-ml-m4-xlarge-%s'% (str(time.time()).replace('.','-'))\n",
    "endpoint_name = SAGEMAKER_ENDPOINT_NAME\n",
    "print('setting up the endpoint..')\n",
    "predictor = predictor_from_estimator(\n",
    "    knn_estimator, \n",
    "    model_name, \n",
    "    instance_type, \n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that we have our predictor, let's use it on our test dataset. The following code runs on the test dataset, computes the accuracy and the average latency. It splits up the data into 100 batches. Then, each batch is given to the inference service to obtain predictions. Once we have all predictions, we compute their accuracy given the true labels of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batches = np.array_split(test_features, 100)\n",
    "print('data split into 100 batches, of size %d.' % batches[0].shape[0])\n",
    "# obtain an np array with the predictions for the entire test set\n",
    "start_time = time.time()\n",
    "predictions = []\n",
    "for batch in batches:\n",
    "    result = predictor.predict(batch)\n",
    "    cur_predictions = np.array([result['predictions'][i]['predicted_label'] for i in range(len(result['predictions']))])\n",
    "    predictions.append(cur_predictions)\n",
    "predictions = np.concatenate(predictions)\n",
    "run_time = time.time() - start_time\n",
    "\n",
    "test_size = test_labels.shape[0]\n",
    "num_correct = sum(predictions == test_labels)\n",
    "accuracy = num_correct / float(test_size)\n",
    "print('time required for predicting %d data point: %.2f seconds' % (test_size, run_time))\n",
    "print('accuracy of model: %.1f%%' % (accuracy * 100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Remember that this sample data set was generated randomly. Therefore, you'll notice the very low accuracy that this model is able to achieve (because there is very little pattern at all within the data being used to create the model).  \n",
    "\n",
    "For your own future use cases using machine learning and SageMaker, it will be up to you to determine the level of accuracy required in order for the model to be beneficial for your application.  Not all use cases require 90+% accuracy in order for benefits to be gained.  Though for some use cases, especially where customer safety or security is part of your application, you may determine that a model must have extreme levels of accuracy in order for it to be leveraged in Production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP!\n",
    "\n",
    "## Mythical Mysfits Workshop Next Steps\n",
    "You have just deployed a prediction endpoint to SageMaker. It can be invoked via HTTP directly.  However, rather than directly have our application frontend integrate with the native SageMaker endpoint, we're going to wrap our own RESTful and serverless API around that prediction endpoint.  Please return to the workshop instructions and proceed to the next step to continue the tutorial. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Clean-Up When Complete with Module 7\n",
    "\n",
    "## Deleting the endpoint\n",
    "\n",
    "We're now done with the example except a final clean-up act. By setting up the endpoint we started a machine in the cloud and as long as it's not deleted the machine is still up and we are paying for it. Once the endpoint is no longer necessary, we delete it. The following code does exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint(predictor):\n",
    "    try:\n",
    "        boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)\n",
    "        print('Deleted {}'.format(predictor.endpoint))\n",
    "    except:\n",
    "        print('Already deleted: {}'.format(predictor.endpoint))\n",
    "\n",
    "delete_endpoint(predictor)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
